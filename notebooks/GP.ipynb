{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires to install eofs and gpytorch\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gpytorch\n",
    "import os\n",
    "import glob\n",
    "from eofs.xarray import Eof\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from typing import Dict, Optional, List, Callable, Tuple, Union\n",
    "\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- As in ClimateBench, we use EOF as a dim reduction technique on the aerosols input (BC and SO2) and keeps only the 5 first modes.  \n",
    "- We use a different version of the Gaussian Process (GP) from the ClimateBench paper. We use a stochastic variational variant (SVGP; see https://arxiv.org/pdf/1411.2005.pdf) that supports training with minibatches and can scale to large datasets. It relies on the library GPytorch (https://gpytorch.ai/ and https://arxiv.org/pdf/1809.11165.pdf), a Pytorch implementation of gaussian processes.\n",
    "- For spatial output, the Linear Model of Coregionalization (LMC) seems to be the best multitask model (run in a reasonable amounts of time; see https://docs.gpytorch.ai/en/stable/examples/04_Variational_and_Approximate_GPs/SVGP_Multitask_GP_Regression.html). The number of latents used is an hyperparameter than control the capacity of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/home/mila/v/venkatesh.ramesh/scratch/causal_data/inputs/input4mips'\n",
    "target_dir = '/home/mila/v/venkatesh.ramesh/scratch/causal_data/targets/CMIP6'\n",
    "\n",
    "models = ['NorESM2-LM']\n",
    "fire_type = 'all-fires'\n",
    "variables = ['pr']\n",
    "train_experiments = [\"ssp585\", \"ssp126\", \"ssp370\"] \n",
    "test_experiments = [\"ssp245\"]\n",
    "input_gases = ['BC_sum', 'CH4_sum', 'CO2_sum', 'SO2_sum']\n",
    "total_ensembles = 1 #-1 for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(mode: str = 'train'):\n",
    "    X, (so2_solver, bc_solver) = get_input_data(input_dir, mode)\n",
    "    y = get_output_data(target_dir, mode)\n",
    "    return torch.tensor(X), torch.tensor(y), (so2_solver, bc_solver)\n",
    "\n",
    "\n",
    "def load_test_data(mode: str = 'train', solvers = None):\n",
    "    X, (so2_solver, bc_solver) = get_input_data(input_dir, mode, solvers)\n",
    "    y = get_output_data(target_dir, mode)\n",
    "    return torch.tensor(X), torch.tensor(y), (so2_solver, bc_solver)\n",
    "\n",
    "\n",
    "def load_data_npz(path: str): #If np data already exists\n",
    "    X_train, y_train = np.load(os.path.join(base_dir, ''))\n",
    "    X_test, y_test = np.load(os.path.join(base_dir, ''))\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def get_input_data(path: str, mode: str, solvers = None, n_eofs : int = 5):\n",
    "    BC = []\n",
    "    CH4 = []\n",
    "    CO2 = []\n",
    "    SO2 = []\n",
    "    \n",
    "    if mode == 'train':      \n",
    "        experiments = train_experiments\n",
    "    elif mode == 'test':\n",
    "        experiments = test_experiments\n",
    "        \n",
    "    for exp in experiments:\n",
    "        for gas in input_gases:\n",
    "            var_dir = os.path.join(path, exp, gas, 'map_250_km/mon')\n",
    "            files = glob.glob(var_dir + '/**/*.nc', recursive=True)\n",
    "\n",
    "            for f in files:\n",
    "                if gas == 'BC_sum' and fire_type in f:\n",
    "                    BC.append(f)\n",
    "            for f in files:\n",
    "                if gas == 'CH4_sum' and fire_type in f:\n",
    "                    CH4.append(f)\n",
    "            for f in files:\n",
    "                if gas == 'BC_sum' and fire_type in f:\n",
    "                    SO2.append(f)\n",
    "            for f in files:\n",
    "                if gas == 'CO2_sum':\n",
    "                    CO2.append(f)\n",
    "\n",
    "    BC_data = xr.open_mfdataset(BC, concat_dim='time', combine='nested').compute().to_array()  # .to_numpy()\n",
    "    SO2_data = xr.open_mfdataset(SO2, concat_dim='time', combine='nested').compute() .to_array()  #.to_numpy()\n",
    "    CH4_data = xr.open_mfdataset(CH4, concat_dim='time', combine='nested').compute().to_array().to_numpy()\n",
    "    CO2_data = xr.open_mfdataset(CO2, concat_dim='time', combine='nested').compute().to_array().to_numpy()\n",
    "    \n",
    "    # BC_data = np.moveaxis(BC_data, 0, 1)\n",
    "    # SO2_data = np.moveaxis(SO2_data, 0, 1)\n",
    "    CH4_data = np.moveaxis(CH4_data, 0, 1)\n",
    "    CO2_data = np.moveaxis(CO2_data, 0, 1)\n",
    "    CH4_data = CH4_data.reshape(CH4_data.shape[0], -1)\n",
    "    CO2_data = CH4_data.reshape(CO2_data.shape[0], -1)\n",
    "\n",
    "    \n",
    "    BC_data = BC_data.transpose('time', 'variable', 'lat', 'lon')\n",
    "    SO2_data = SO2_data.transpose('time', 'variable', 'lat', 'lon')\n",
    "    BC_data = BC_data.assign_coords(time=np.arange(len(BC_data.time)))\n",
    "    SO2_data = SO2_data.assign_coords(time=np.arange(len(SO2_data.time)))\n",
    "\n",
    "    \n",
    "    # Compute EOFs for BC\n",
    "    if solvers is None:\n",
    "        # print(BC_data.shape)\n",
    "        bc_solver = Eof(BC_data)\n",
    "        bc_eofs = bc_solver.eofsAsCorrelation(neofs=n_eofs)\n",
    "        bc_pcs = bc_solver.pcs(npcs=n_eofs, pcscaling=1)\n",
    "\n",
    "        # Compute EOFs for SO2\n",
    "        so2_solver = Eof(SO2_data)\n",
    "        so2_eofs = so2_solver.eofsAsCorrelation(neofs=n_eofs)\n",
    "        so2_pcs = so2_solver.pcs(npcs=n_eofs, pcscaling=1)\n",
    "\n",
    "        print(bc_pcs)\n",
    "\n",
    "        # Convert to pandas\n",
    "        bc_df = bc_pcs.to_dataframe().unstack('mode')\n",
    "        bc_df.columns = [f\"BC_{i}\" for i in range(n_eofs)]\n",
    "\n",
    "        so2_df = so2_pcs.to_dataframe().unstack('mode')\n",
    "        so2_df.columns = [f\"SO2_{i}\" for i in range(n_eofs)]\n",
    "    else:\n",
    "        so2_solver = solvers[0]\n",
    "        bc_solver = solvers[1]\n",
    "        \n",
    "        so2_pcs = so2_solver.projectField(SO2_data, neofs=n_eofs, eofscaling=1)\n",
    "        so2_df = so2_pcs.to_dataframe().unstack('mode')\n",
    "        so2_df.columns = [f\"SO2_{i}\" for i in range(n_eofs)]\n",
    "\n",
    "        bc_pcs = bc_solver.projectField(BC_data, neofs=n_eofs, eofscaling=1)\n",
    "        bc_df = bc_pcs.to_dataframe().unstack('mode')\n",
    "        bc_df.columns = [f\"BC_{i}\" for i in range(n_eofs)]\n",
    "    \n",
    "    CH4_data = CH4_data[:, :1]\n",
    "    CO2_data = CO2_data[:, :1]\n",
    "\n",
    "    print(bc_df.shape)\n",
    "    print(CH4_data.shape)\n",
    "    print(CO2_data.shape)\n",
    "    print(so2_df.shape)\n",
    "    \n",
    "    merged_data = np.concatenate((bc_df, CH4_data, CO2_data, so2_df), axis=1)\n",
    "    return merged_data, (so2_solver, bc_solver)\n",
    "\n",
    "\n",
    "def get_output_data(path: str, mode: str):\n",
    "    nc_files = []\n",
    "    \n",
    "    if mode == 'train':\n",
    "        experiments = train_experiments\n",
    "    elif mode == 'test':\n",
    "        experiments = test_experiments\n",
    "        \n",
    "    for mod in models:\n",
    "\n",
    "        model_dir = os.path.join(path, mod)\n",
    "        ensembles = os.listdir(model_dir)\n",
    "\n",
    "        if total_ensembles == 1:\n",
    "            ensembles = ensembles[0]\n",
    "        \n",
    "        exp_counter = 0\n",
    "        for exp in experiments:\n",
    "            for var in variables:\n",
    "                var_dir = os.path.join(path, mod, ensembles, exp, var, '250_km/mon')\n",
    "                files = glob.glob(var_dir + '/**/*.nc', recursive=True)\n",
    "                nc_files += files\n",
    "        \n",
    "            if exp_counter == 0:\n",
    "                dataset = xr.open_mfdataset(nc_files).compute().to_array().to_numpy()\n",
    "        \n",
    "            else: #concatenate dataset in time dimension\n",
    "                other_experiment = xr.open_mfdataset(nc_files).compute().to_array().to_numpy()\n",
    "                dataset = np.concatenate((dataset, other_experiment), axis=1)\n",
    "                \n",
    "                \n",
    "            exp_counter += 1\n",
    "            \n",
    "        dataset = np.moveaxis(dataset, 0, 1)\n",
    "        print(dataset.shape)\n",
    "        dataset = dataset.reshape(dataset.shape[0], -1)\n",
    "        \n",
    "        # TODO: remove next line, only used for making quick tests\n",
    "        dataset = dataset[:, :1]\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'pcs' (time: 3096, mode: 5)>\n",
      "array([[-1.3052769 , -1.2704228 ,  0.41740844, -1.1416101 , -2.3582041 ],\n",
      "       [-1.1239995 , -0.90367   ,  0.21111041, -1.577814  , -0.45786643],\n",
      "       [-0.93241996, -0.81854814,  0.19349629, -1.7038045 ,  0.5746188 ],\n",
      "       ...,\n",
      "       [-0.7419013 ,  0.5875077 ,  1.0679767 ,  1.3232024 ,  1.6523781 ],\n",
      "       [-0.6475205 , -0.62544924,  0.31076536,  1.3729664 ,  1.119416  ],\n",
      "       [-0.67845637, -0.8426438 ,  0.26810127,  1.4787649 , -0.32489872]],\n",
      "      dtype=float32)\n",
      "Coordinates:\n",
      "  * time     (time) int64 0 1 2 3 4 5 6 7 ... 3089 3090 3091 3092 3093 3094 3095\n",
      "  * mode     (mode) int64 0 1 2 3 4\n",
      "(3096, 5)\n",
      "(3096, 1)\n",
      "(3096, 1)\n",
      "(3096, 5)\n",
      "(3096, 1, 96, 144)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, (so2_solver, bc_solver) = load_train_data('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1032, 5)\n",
      "(1032, 1)\n",
      "(1032, 1)\n",
      "(1032, 5)\n",
      "(1032, 1, 96, 144)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test, (so2_solver, bc_solver) = load_test_data('test', (so2_solver, bc_solver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for test:\n",
    "class ClimateDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # global\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "training_data = ClimateDataset(X_train, y_train)\n",
    "test_data = ClimateDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative dataset, just used for test:\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, n=3000):\n",
    "        # global\n",
    "        self.X = torch.rand([n, 1440])  # 4\n",
    "        self.y = torch.rand([n, 1440])  # 2\n",
    "        \n",
    "        # spatial\n",
    "        # self.X = torch.rand([n, 4 * 96 * 144])\n",
    "        # self.y = torch.rand([n, 1 * 96 * 144])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# training_data = DummyDataset(n=500)\n",
    "# test_data = DummyDataset(n=100)\n",
    "\n",
    "# train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Model (GP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_inducing_points = 500\n",
    "n_epochs = 2  # Could use criterion to stop\n",
    "lr = 0.1\n",
    "# optimizer = adam\n",
    "# kernel = matern3/2\n",
    "\n",
    "class ApproxGPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, num_tasks):\n",
    "        # inducing_points size: num_outputs, num_examples, num_features\n",
    "        inducing_points = inducing_points.reshape(1, inducing_points.size(0), -1)\n",
    "        # inducing_points = inducing_points.repeat(num_tasks, 1, 1)\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(-2), batch_shape=torch.Size([num_tasks]))\n",
    "\n",
    "        variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ),\n",
    "            num_tasks=num_tasks\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([num_tasks]))\n",
    "\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, batch_shape=torch.Size([num_tasks])), batch_shape=torch.Size([num_tasks]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "class GaussianProcess(nn.Module):\n",
    "    def __init__(self,\n",
    "                 inducing_points,\n",
    "                 num_out_var):\n",
    "        super().__init__()\n",
    "        self.model = ApproxGPModel(inducing_points=inducing_points, num_tasks=num_out_var)\n",
    "        self.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_out_var)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        predictions = self.likelihood(self.model(X))\n",
    "        return predictions.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 12])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.X[:num_inducing_points].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_model = GaussianProcess(training_data.X[:num_inducing_points], training_data.y.size(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianProcess(\n",
       "  (model): ApproxGPModel(\n",
       "    (variational_strategy): IndependentMultitaskVariationalStrategy(\n",
       "      (base_variational_strategy): VariationalStrategy(\n",
       "        (_variational_distribution): CholeskyVariationalDistribution()\n",
       "      )\n",
       "    )\n",
       "    (mean_module): ConstantMean()\n",
       "    (covar_module): ScaleKernel(\n",
       "      (base_kernel): MaternKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "  )\n",
       "  (likelihood): MultitaskGaussianLikelihood(\n",
       "    (raw_task_noises_constraint): GreaterThan(1.000E-04)\n",
       "    (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "    {'params': gp_model.parameters()}\n",
    "], lr=lr)\n",
    "mll = gpytorch.mlls.VariationalELBO(gp_model.likelihood, gp_model.model, num_data=training_data.y.size(0))\n",
    "gp_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([24, 1])\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_epochs):\n",
    "    print(f\"epoch #{i}\")\n",
    "    for x, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = gp_model(x)\n",
    "        loss = -mll(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "gp_model.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    predictions = gp_model.likelihood(gp_model(test_data.X))\n",
    "    y_pred = predictions.mean\n",
    "    # lower, upper = predictions.confidence_region()  # could have confidence regions if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005023047"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred = y_pred.reshape(3096, 1, 96, 144)\n",
    "rmse = mean_squared_error(test_data.y, y_pred, squared=False)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

# @package _global_
# to execute this experiment run:
# python run.py experiment=example

defaults:
  - override /mode: exp.yaml
  - override /trainer: default.yaml
  - override /model: climax_frozen.yaml # put the desired model name here
  - override /callbacks: default.yaml
  - override /logger: wandb.yaml
  - override /datamodule: climate_super.yaml # set to super emulation
  - override /decoder: multihead_decoder.yaml # include multi-model decoder

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# can also be accessed by loggers
run: 
  exp_num: "02" # serial number of experiment
  name: A100 # a short 1-2 word description of the experiment
  comment: "A100"

name: ${run.exp_num}_${model.model_name}_${run.name}_


seed: 99 #42 #7

trainer:
  min_epochs: 1
  max_epochs: 100
  #gradient_clip_val: 5

model:
  model_name: climax_frozen
  loss_function: "climax_lon_lat_rmse"
  monitor: "val/llrmse_climax"
  finetune: False 
  pretrained_run_id:  "bcvgpg4b"
  pretrained_ckpt_dir: "home/causalpaca/emulator/emulator/bcvgpg4b/checkpoints/epoch=8-step=180.ckpt"
  pretrained_run_id_decoder: "xog84o5g"
  pretrained_ckpt_dir_decoder: "home/causalpaca/emulator/emulator/xog84o5g/checkpoints/epoch=8-step=180.ckpt"
  optimizer: # need that for decoder
    is_filtered: True
 

datamodule: # overwrite what stuff to train on
  out_var_ids: ['pr', 'tas']
  seq_to_seq: True # determine the task setting
  batch_size: 2
  channels_last: False
  eval_batch_size: 2
  seq_len: 12
  in_var_ids : ['BC_sum','SO2_sum', 'CO2_sum', 'CH4_sum']
  train_years: "2015-2100"
  train_scenarios: ["ssp126", "ssp370", "ssp585"]
  test_scenarios : ["ssp245"]
  test_models : ["MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR","MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR","MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR","MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR","MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR","MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR"]
  train_models : ["MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR","MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR","MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR","MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR","MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR","MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR"]

 
logger:
  wandb:
    tags: ["climax_frozen", "super_emulation", "larger_decoder", "36_models", "run2","A100"] # set your tags here
    group: "Baseline"

# @package _global_
# to execute this experiment run:
# python run.py experiment=example

defaults:
  - override /mode: exp.yaml
  - override /trainer: MPT.yaml
  - override /model: conv_lstm.yaml # put the desired model name here
  - override /callbacks: default.yaml
  - override /logger: wandb.yaml
  - override /datamodule: climate_super.yaml # set to super emulation
  - override /decoder: multihead_decoder.yaml # include multi-model decoder

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# can also be accessed by loggers
run: 
  exp_num: "NW0" # serial number of experiment
  name: NW0 #a short 1-2 word description of the experiment
  comment: "A100"

name: ${run.name}_${model.model_name}_${datamodule.batch_size}_

seed: 42 #99 #7

trainer:
  min_epochs: 1
  max_epochs: 100
  #gradient_clip_val: 5

model:
  model_name: convlstm
  loss_function: "climax_lon_lat_rmse"
  monitor: "val/llrmse_climax"
  finetune: False 
  pretrained_run_id:  "bcvgpg4b"
  pretrained_ckpt_dir: "home/causalpaca/emulator/emulator/bcvgpg4b/checkpoints/epoch=8-step=180.ckpt"
  pretrained_run_id_decoder: "xog84o5g"
  pretrained_ckpt_dir_decoder: "home/causalpaca/emulator/emulator/xog84o5g/checkpoints/epoch=8-step=180.ckpt"
  optimizer: # need that for decoder
    is_filtered: True
 

datamodule: # overwrite what stuff to train on
  pin_memory: True
  num_workers: 0
  out_var_ids: ['pr', 'tas']
  seq_to_seq: True # determine the task setting
  batch_size: 8
  channels_last: False 
  eval_batch_size: 8
  seq_len: 12
  in_var_ids : ['BC_sum','SO2_sum', 'CO2_sum', 'CH4_sum']
  train_years: "2015-2100"
  train_scenarios: ["ssp126", "ssp370", "ssp585"]
  test_scenarios : ["ssp245"]
  #train_models : ['AWI-CM-1-1-MR', 'BCC-CSM2-MR', 'CAMS-CSM1-0', 'CAS-ESM2-0', 'CESM2', 'CESM2-WACCM', 'CMCC-CM2-SR5', 'CMCC-ESM2', 'CNRM-CM6-1-HR', 'EC-Earth3', 'EC-Earth3-Veg', 'EC-Earth3-Veg-LR', 'FGOALS-f3-L', 'GFDL-ESM4', 'INM-CM4-8', 'INM-CM5-0', 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-LM', 'NorESM2-MM', 'TaiESM1'] #["MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR"]
  #test_models: ['AWI-CM-1-1-MR', 'BCC-CSM2-MR', 'CAMS-CSM1-0', 'CAS-ESM2-0', 'CESM2', 'CESM2-WACCM', 'CMCC-CM2-SR5', 'CMCC-ESM2', 'CNRM-CM6-1-HR', 'EC-Earth3', 'EC-Earth3-Veg', 'EC-Earth3-Veg-LR', 'FGOALS-f3-L', 'GFDL-ESM4', 'INM-CM4-8', 'INM-CM5-0', 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-LM', 'NorESM2-MM', 'TaiESM1'] #["MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR"]
  train_models : ["MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR"]
  test_models: ["MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR"]

logger:
  wandb:
    # tags: ["conv_lstm", "super_emulation", "decoder", "6_models", "run2","RTX8000"] # set your tags here
    tags: ["TEST"]
    group: ${run.name}_${model.model_name} 


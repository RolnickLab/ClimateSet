# @package _global_
# to execute this experiment run:
# python run.py experiment=example

defaults:
  - override /mode: exp.yaml
  - override /trainer: MPT.yaml
  - override /model: conv_lstm.yaml # put the desired model name here
  - override /callbacks: default.yaml
  - override /logger: wandb.yaml
  - override /datamodule: climate_super.yaml # set to super emulation
  - override /decoder: multihead_decoder.yaml # include multi-model decoder


# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# can also be accessed by loggers
run: 
  exp_num: "TEST" # serial number of experiment
  name: PytorchLogger # a short 1-2 word description of the experiment
  comment: "Testing the convlstm model with the tensorboard pytorch profiler logger."

name: ${run.exp_num}_${model.model_name}_${run.name}_
pyprofile: True

seed: 99 #42 #7

trainer:
  min_epochs: 1
  max_epochs: 20
  #gradient_clip_val: 5

model:
  model_name: convlstm
  loss_function: "climax_lon_lat_rmse"
  monitor: "val/llrmse_climax"
  finetune: False 
  pretrained_run_id:  "bcvgpg4b"
  pretrained_ckpt_dir: "home/causalpaca/emulator/emulator/bcvgpg4b/checkpoints/epoch=8-step=180.ckpt"
  pretrained_run_id_decoder: "xog84o5g"
  pretrained_ckpt_dir_decoder: "home/causalpaca/emulator/emulator/xog84o5g/checkpoints/epoch=8-step=180.ckpt"
  optimizer: # need that for decoder
    is_filtered: True
 

datamodule: # overwrite what stuff to train on
  out_var_ids: ['pr', 'tas']
  seq_to_seq: True # determine the task setting
  batch_size: 16
  channels_last: False 
  eval_batch_size: 16
  seq_len: 12
  in_var_ids : ['BC_sum','SO2_sum', 'CO2_sum', 'CH4_sum']
  train_years: "2015-2100"
  train_scenarios: ["ssp126", "ssp370", "ssp585"]
  test_scenarios : ["ssp245"]
  #train_models : ['AWI-CM-1-1-MR', 'BCC-CSM2-MR', 'CAMS-CSM1-0', 'CAS-ESM2-0', 'CESM2', 'CESM2-WACCM', 'CMCC-CM2-SR5', 'CMCC-ESM2', 'CNRM-CM6-1-HR', 'EC-Earth3', 'EC-Earth3-Veg', 'EC-Earth3-Veg-LR', 'FGOALS-f3-L', 'GFDL-ESM4', 'INM-CM4-8', 'INM-CM5-0', 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-LM', 'NorESM2-MM', 'TaiESM1'] #["MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR"]
  #test_models: ['AWI-CM-1-1-MR', 'BCC-CSM2-MR', 'CAMS-CSM1-0', 'CAS-ESM2-0', 'CESM2', 'CESM2-WACCM', 'CMCC-CM2-SR5', 'CMCC-ESM2', 'CNRM-CM6-1-HR', 'EC-Earth3', 'EC-Earth3-Veg', 'EC-Earth3-Veg-LR', 'FGOALS-f3-L', 'GFDL-ESM4', 'INM-CM4-8', 'INM-CM5-0', 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-LM', 'NorESM2-MM', 'TaiESM1'] #["MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR"]
  train_models : ["MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR"]
  test_models: ["MPI-ESM1-2-HR", "AWI-CM-1-1-MR", "NorESM2-LM", "FGOALS-f3-L", "EC-Earth3", "BCC-CSM2-MR"]

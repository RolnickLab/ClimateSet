#!/bin/bash

#SBATCH --job-name="BS64-SC6-A100-basetest-unet"

#SBATCH --nodes=1

#SBATCH --gpus-per-node=a100:1 --constraint="dgx&ampere"       # specify gpu

#SBATCH --ntasks-per-node=1

#SBATCH --cpus-per-task=4       # cpu-cores per task (>1 if multi-threaded tasks)

#SBATCH --mem-per-cpu=16G               # memory per node (4G per cpu-core is default)

#SBATCH --time=00:30:00                                  # set runtime

#SBATCH -o /home/mila/f/felix-andreas.nahrstedt/Slurm/BS64-SC6-A100-basetest-unet.out        # set log dir to home

module load python/3.10
export PYTHONPATH=$(pwd)

# 2. Load DL Framework

module load cuda/10.0/cudnn/7.6
module load python/3.7/cuda/11.1/cudnn/8.0/pytorch/1.8.1
    

# 3. Create or Set Up Environment

source env_new_emulator/bin/activate
echo "activated"

echo $PYTHONPATH
dir
cd $(pwd)


export NCCL_BLOCKING_WAIT=1 #Pytorch Lightning uses the NCCL backend for inter-GPU communication by default. Set this variable to avoid timeout errors.


# 8. Run Python
export HYDRA_FULL_ERROR=1
echo "Running python test.py ..."
srun python emulator/run.py experiment=Batchsize_A100_Experiments_unet/superemulator_unet_BS64.yaml seed=3423


# 9. Copy output to scratch
#cp /home/mila/f/felix-andreas.nahrstedt/Slurm/SC6-A100-basetest-unet_out.out 

# 10. Experiment is finished

echo "Experiment $EXP_NAME is concluded."